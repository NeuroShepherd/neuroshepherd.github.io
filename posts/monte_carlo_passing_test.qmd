---
title: "Monte Carlo Simulation of Passing a Test"
description: "I had to take a test recently, and I wanted to see if I could stop studying already..."
date: 2025-08-06
year: 2025
date-modified: last-modified
image: ../images/placeholder.png
title-block-banner: "#8a14d3ff" # this can be a different color or an image
draft: false
categories:
    - Monte Carlo
    - Simulation
    - R
---


```{r}
library(purrr)

library(ggplot2)
library(dplyr)
```

## Context

I recently had to take a multiple choice test, and I was, frankly, tired of studying for it. The test wasn't particularly difficult, but it did require quite a bit of rote memorization of dates, historical events, and other facts. Fortunately, this test's parameters are very well defined: there's a bank of ~300 possible, publicly-available questions, the test is generated by randomly selecting 33 of those questions, and to pass the test, you only need to get over 50% of the questions correct, which means you can answer *just* 17 questions correctly to pass.

As I was in the middle of my vacation and studying for this test, I kept asking myself "Haven't I studied enough already?" And so I decided to get an answer to that question with simulations, specifically a Monte Carlo simulation. More precisely, I wanted to find out what percentage of the time I would pass the actual test based on my current level of knowledge, as represented by the total number of practice question I had answered correctly. 

## Monte Carlo Simulations

First things first: what is a Monte Carlo simulation? In short, it's a statistical technique that uses random sampling to obtain numerical results. The idea is to run many simulations (or "trials") and analyze the results to understand the behavior of the system. In this case, I wanted to simulate the process of taking the test multiple times, each time randomly selecting answers based on my current knowledge level. The key steps in a Monte Carlo simulation are:

1. **Define the Problem**: In this case, the problem is to determine the probability of passing the test based on my current knowledge level.
2. **Model the System**: Create a model that represents the system being studied. Here, the model is the test-taking process, where I randomly select answers based on my knowledge level.
3. **Run Simulations**: Execute the model multiple times, each time with different random inputs to simulate the variability in the system.
4. **Analyze Results**: Collect and analyze the results to draw conclusions about the system's behavior.

With a very basic understanding of Monte Carlo simulation in hand, let's take a look at the precise parameters I used for my simulation and the code to run it.

## Simulation Parameters and Code

I was working with a total of 308 possible questions, and I had answered 250 of those correctly (mostly through actual knowledge, but a handful through educated guessing or simple chance). The test again consists of 33 randomly selected questions, and I needed to answer at least 17 of those correctly to pass. With those parameters in mind, my chances of passing the test already seem intuitively good as I had answered `r round(250/308,4)*100`% of the questions correctly. But I wanted to see how that translated into actual test-taking scenarios, so I wrote the following R code to run the simulation:

```{r}
#| lst-label: lst-monte-carlo-simulation
#| lst-cap: "Simple Monte Carlo simulation for passing the test"
#| code-fold: show

# Set parameters
total_questions <- 308
correct_answers <- 250
test_questions <- 33
passing_score <- 17
n_simulations <- 100000


# create a vector to represent practice results
practice_results <- c(rep(1, correct_answers), rep(0, total_questions - correct_answers))

# sample 33 (i.e. test_questions) of those questions without replacement, and replicate this 10000 times
simulation_results <- replicate(n_simulations, {
  sum(sample(practice_results, test_questions, replace = FALSE))
})


```

So what exactly is happening in @lst-monte-carlo-simulation? First, I set the parameters for the simulation, including the total number of questions, the number of questions I had answered correctly, the number of questions on the test, the passing score, and the number of simulations to run.

Next, I created a vector called `practice_results` that represents my practice results, where `1` indicates a correct answer and `0` indicates an incorrect answer. This vector is constructed by repeating `1` for the number of correct answers and `0` for the remaining questions.

Finally, I used the `replicate()` function to run the simulation `n_simulations` times. In each iteration, I randomly sampled 33 questions from the `practice_results` vector without replacement and summed the number of correct answers. The result is stored in the `simulation_results` vector.

In short then, I have a vector of 10,000 test results, each representing the number of questions I answered correctly on a simulated test. Now, I can analyze these results to determine the probability of passing the test.

## Analyzing the Results

A simple first step is to plot the distribution of the number of correct answers across all simulations. This will give us a visual representation of how many times I passed the test.


```{r}
#| label: fig-simulation-results-histogram
#| fig-cap: "Distribution of Correct Answers in Simulated Tests"

data.frame(correct_answers = simulation_results) %>%
mutate(passed = correct_answers >= 17) %>%
ggplot(aes(x = correct_answers, fill = passed)) +
  geom_histogram(binwidth = 1, color = "#241a1aff", alpha = 0.7) +
  labs(title = "Distribution of Correct Answers in Simulated Tests",
       x = "Number of Correct Answers",
       y = "Frequency") +
  geom_vline(xintercept = 17, linetype = "dashed", color = "red", size = 1) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 20, face = "bold"),
    axis.text = element_text(size = 14),
    axis.title = element_text(size = 16)
  ) +
  scale_x_continuous(limits = c(0, 33)) +
  scale_fill_manual(values = c("TRUE" = "#8ef4a7ff", "FALSE" = "#f48e8eff"))
```

From the histogram alone, we can see that nearly all of the simulated tests resulted in a passing score, with the vast majority of results clustering around 25 correct answers. Any reasonable interpretation of this histogram would suggest that I should be able to pass the test with a high degree of confidence. But let's quantify it further by calculating the percentage of simulations that resulted in a passing score.


```{r}
#| label: passing-rate
#| fig-cap: "Percentage of Simulations Resulting in a Passing Score"
#| code-fold: show

passing_rate <- mean(simulation_results >= passing_score) * 100
```


This yields a passing rate of `r round(passing_rate, 5)`%...I think I can stop studying now!


## I Could Have Studied Less

But when *could* I have stopped studying? To answer this, we'll need to re-run the simulation with different numbers of correct answers on the practice test. Specifically, we'll run the simulation for every possible number of correct answers from 0 to 308 and calculate the passing rate for each case.

First, let's set up the simulation for all possible practice test scores:


```{r}
#| lst-label: lst-monte-carlo-simulation-all-levels
#| lst-cap: "Monte Carlo simulation for all practice test scores"
#| cache: true
#| code-fold: show


# Create empty matrix to hold results
results_mat <- matrix(0, 308, 308) 
# Fill lower triangle with 1s to represent valid practice test scores
results_mat[lower.tri(results_mat)] <- 1
# Transpose the matrix to have practice test scores as rows and simulation results as columns
results_mat <- t(results_mat)
# add last column for case of all correct on practice test
results_mat <- cbind(results_mat, 1)



# Run simulations for all practice test scores
simulation_all_levels <- results_mat %>%
  as.data.frame() %>%
  tibble::tibble() %>%
  purrr::map(
    ~replicate(
      10000, 
      sum(sample(.x, 33, replace = FALSE))
    )
  )


```

@lst-monte-carlo-simulation-all-levels can be thought of in a few different steps, First, I create an empty matrix with 308 rows and 308 columns, where each row represents a different number of correct answers on the practice test (from 0 to 307--not to 308, but we'll clarify this in a moment)). The lower triangle of the matrix is then filled with `1`s which means that each row represents a test score with one more correct answer than the previous row. However, I don't like to have the rows representing the practice tests, and find it preferable to have the practice test scores as columns, so I transpose the matrix with `t()`. Finally, I add a last column to represent the case where all questions on the practice test were answered correctly (i.e., 308 correct answers).

With the data matrix now set up, we can essentially recreate the simulations we ran in @lst-monte-carlo-simulation, but this time for every possible number of correct answers on the practice test. This means mapping over each column of the data, and running the simulation for each practice test score. The result is a list of vectors, where each vector contains the results of 10,000 simulated tests for a specific number of correct answers on the practice test.

So, what can we do with these results? We can calculate the probability of passing th test for each number of correct answers on the practice test, and then plot the results to see how the probability of passing changes as the number of correct answers increases.


```{r}
#| label: fig-passing-probability
#| fig-cap: "Probability of Passing the Test Based on Practice Test Scores"
#| fig-width: 12
#| fig-height: 8


simulation_all_levels %>%
  map_dbl(
    ~sum(.x >= 17)/10000
  ) %>%
  tibble::tibble(passig_probability = .) %>%
  mutate(number_correct_practice = row_number()) %>%
  ggplot(aes(number_correct_practice, passig_probability)) +
  geom_point() +
  geom_line() +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 20, face = "bold"),
    axis.text = element_text(size = 14),
    axis.title = element_text(size = 16)
  ) +
  labs(
    title = "Probability of Passing Test Based on Practice Test Scores",
    x = "Number of Correct Answers on Practice Test",
    y = "Probability of Passing"
  )
```


Let's add in some confidence intervals


```{r}
results_with_ci <- simulation_all_levels %>%
  set_names(0:(length(simulation_all_levels) - 1)) %>%  # Name the list elements
  map_dfr(
    ~{
      successes <- sum(.x >= 17)
      trials <- length(.x)
      proportion <- successes / trials
      ci <- binom.test(successes, trials)$conf.int
      tibble(
        passing_probability = proportion,
        lower_ci = ci[1],
        upper_ci = ci[2]
      )
    },
    .id = "number_correct_practice"
  ) %>%
  mutate(number_correct_practice = as.numeric(number_correct_practice))



ribbon_plot <- results_with_ci %>%
  ggplot(aes(x = number_correct_practice, y = passing_probability)) +
  geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci), alpha = 0.4, fill = "blue") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 20, face = "bold"),
    axis.text = element_text(size = 14),
    axis.title = element_text(size = 16)
  ) +
  labs(
    title = "Probability of Passing Test with Confidence Intervals",
    x = "Number of Correct Answers on Practice Test",
    y = "Probability of Passing"
  ) 


plotly::ggplotly(ribbon_plot)


```
